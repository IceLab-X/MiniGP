{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Author:** Zidong Chen\n",
    "\n",
    "**Introduction:** This notebook explains why non-positive-definite or Nan error occurs during GP model training, a problem should not happen in theory. If you are aware of floating point error and IEEE754 standard, you can skip this notebook.\n",
    "\n",
    "Let's start with a simple but mind-blowing example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a == b: False\n"
     ]
    }
   ],
   "source": [
    "a = 0.1 + 0.2\n",
    "b = 0.3\n",
    "print(\"a == b:\", a == b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T07:28:30.003683500Z",
     "start_time": "2024-07-30T07:28:29.995414400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result is False. It is because the floating-point number is represented in binary in the computer, and the binary representation of 0.1, 0.2, and 0.3 are not exact. The actual values are:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 0.30000000000000004\n",
      "b: 0.3\n",
      "Difference: 5.551115123125783e-17\n"
     ]
    }
   ],
   "source": [
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"Difference:\", abs(a - b))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T07:28:31.010321800Z",
     "start_time": "2024-07-30T07:28:30.999328500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-200=0 is: False\n",
      "1e-2000=0 is: True\n"
     ]
    }
   ],
   "source": [
    "print(\"1e-200=0 is:\", 1e-200==0)\n",
    "print(\"1e-2000=0 is:\", 1e-2000==0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T07:27:52.131748500Z",
     "start_time": "2024-07-30T07:27:52.105417800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The IEEE 754 standard is a widely adopted set of rules for binary floating-point arithmetic in computers. It was established by the Institute of Electrical and Electronics Engineers (IEEE) to ensure consistency and portability of numerical computations across different computing systems. Hereâ€™s a detailed overview:\n",
    "### Key Features of IEEE 754 Standard\n",
    "\n",
    "#### Representation of Floating Point Numbers\n",
    "\n",
    "- **Sign Bit**: Determines if the number is positive or negative.\n",
    "- **Exponent**: Represents the power to which the base (usually 2) is raised.\n",
    "- **Mantissa (or Significand)**: Represents the significant digits of the number.\n",
    "\n",
    "#### Floating Point Formats\n",
    "\n",
    "- **Single Precision (32-bit)**\n",
    "  - 1 bit for the sign\n",
    "  - 8 bits for the exponent\n",
    "  - 23 bits for the mantissa\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def float_to_ieee754_components(f):\n",
    "    # Convert the float to raw binary representation\n",
    "    binary_rep = struct.unpack('>I', struct.pack('>f', f))[0]\n",
    "\n",
    "    # Extract sign (1 bit), exponent (8 bits), and significand (23 bits)\n",
    "    sign = (binary_rep >> 31) & 0x1\n",
    "    exponent = (binary_rep >> 23) & 0xFF\n",
    "    significand = binary_rep & 0x7FFFFF\n",
    "\n",
    "    # Compute the actual exponent by subtracting the bias (127 for single precision)\n",
    "    actual_exponent = exponent - 127\n",
    "\n",
    "    # Convert the significand to the normalized form by adding the implicit leading 1\n",
    "    if exponent != 0:\n",
    "        normalized_significand = 1 + significand / (2**23)\n",
    "    else:\n",
    "        # Handle the case for denormals\n",
    "        normalized_significand = significand / (2**23)\n",
    "\n",
    "    return sign, actual_exponent, normalized_significand\n",
    "\n",
    "def print_float_components(f):\n",
    "    sign, exponent, significand = float_to_ieee754_components(f)\n",
    "    print(f\"Floating-point number: {f}\")\n",
    "    print(f\"Sign: {sign}\")\n",
    "    print(f\"Exponent (actual): {exponent}\")\n",
    "    print(f\"Significand (normalized): {significand}\")\n",
    "    print(f\"Representation: (-1)^{sign} * {significand} * 2^{exponent}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T07:21:54.626543700Z",
     "start_time": "2024-07-30T07:21:54.619121400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating-point number: 0.1\n",
      "Sign: 0\n",
      "Exponent (actual): -4\n",
      "Significand (normalized): 1.600000023841858\n",
      "Representation: (-1)^0 * 1.600000023841858 * 2^-4\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "float_number = 0.1\n",
    "print_float_components(float_number)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T02:46:15.668660900Z",
     "start_time": "2024-07-19T02:46:15.657779300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "0.10000000149011612"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1)**0 * 1.600000023841858 * 2**(-4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-30T07:50:43.653091500Z",
     "start_time": "2024-07-30T07:50:43.642096400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Catastrophic Cancellation:** This occurs when subtracting two nearly equal numbers, resulting in a significant loss of precision. The significant digits cancel out, leaving behind the less significant, error-prone digits. This can happen quite often in kernel computations, leading to non-positive-definite matrices or NaN errors during GP model training. This also explains why remove similar data points can help fix the problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
